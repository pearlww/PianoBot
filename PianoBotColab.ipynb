{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PianoBotColab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tWFoG8TgbvY"
      },
      "source": [
        "# Piano Bot notebook\n",
        "This notebook was made for training the models on Google Colab. It works by mounting the personal google drive of the user, selecting a folder (in the fashion of a workspace) and cloning the github repo there. All the code is in the python files, so in this notebook there's just a sequence of commands to get it all up and running. \n",
        "\n",
        "The graphics for the training and evaluation loss are displayed on tensorboard. The easiest way to access tensorboard is on the local PC, since the default behaviour is to open a server on localhost that can be accessed on the web browser. It is advised then to download the logs folder when the training is done, and run the tensorboard command in that folder to create the tensorboard server.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W59HGsf1rLNO",
        "outputId": "cab1c6a6-2949-4ea3-a052-c603642bb25c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHyE7LXirpai",
        "outputId": "86e3000e-3076-44b6-b411-cdb90099d9a9"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeWBEL_VbVcT"
      },
      "source": [
        "Specify your drive path here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6hRDeYKm2a",
        "outputId": "de2116c0-c590-4ffb-e3b0-66bfc23aa96a"
      },
      "source": [
        "%cd gdrive/My Drive/Universidad/DTU/Deep_Learning/Project/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Universidad/DTU/Deep_Learning/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09DAkxawbdLI"
      },
      "source": [
        "Clone github (with the data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn-OjOdRs-bi",
        "outputId": "2e706af7-9e3c-4c88-c2ec-ce42a6a9220b"
      },
      "source": [
        "! git clone https://github.com/pearlww/PianoBot.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'PianoBot' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMp_-7VPb6JP"
      },
      "source": [
        "Required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6JNWpMexp7J",
        "outputId": "fb03af60-2e97-40b5-bbc6-a32e5e2222e1"
      },
      "source": [
        "%cd PianoBot/\n",
        "! pip install tensorboardX\n",
        "! pip install pretty_midi\n",
        "! pip install progress\n",
        "! pip install timidity"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Universidad/DTU/Deep_Learning/Project/PianoBot\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.0.0)\n",
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.6/dist-packages (0.2.9)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.2.9)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.15.0)\n",
            "Requirement already satisfied: progress in /usr/local/lib/python3.6/dist-packages (1.5)\n",
            "Requirement already satisfied: timidity in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: simpleaudio in /usr/local/lib/python3.6/dist-packages (from timidity) (1.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from timidity) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from timidity) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIBSsJkfb_S4"
      },
      "source": [
        "All parameters are set in config.py.\n",
        "We developed two main models: the basic transformer from \"Attention is all you need\" paper and the music transformer from the paper with the same name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGMSmS-QFUbf"
      },
      "source": [
        "# import our defined files\n",
        "import config\n",
        "from music_transformer import MusicTransformer\n",
        "from transformer import Transformer\n",
        "import loss_functions\n",
        "from data_loader import DataLoader\n",
        "import utils\n",
        "\n",
        "import datetime\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjcrUjVvFsZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbcc38e-b7b3-4c29-a2be-ac9eee43e2fd"
      },
      "source": [
        "# check cuda\n",
        "if torch.cuda.is_available():\n",
        "    config.device = torch.device('cuda')\n",
        "else:\n",
        "    config.device = torch.device('cpu')\n",
        "print('| Summary - Device Info : {}'.format(torch.cuda.device))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Summary - Device Info : <class 'torch.cuda.device'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q0FjHf5GCbh"
      },
      "source": [
        "#define model as music transformer or normal transformer\n",
        "model = MusicTransformer(\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            vocab_size=config.vocab_size,\n",
        "            num_layer=config.num_layers,\n",
        "            max_seq= config.max_seq,\n",
        "            dropout=config.dropout,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE8970dEGHy-"
      },
      "source": [
        "model = Transformer(\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            vocab_size=config.vocab_size,\n",
        "            num_layer=config.num_layers,\n",
        "            max_seq= config.max_seq,\n",
        "            dropout=config.dropout,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ01NCimGJNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c3ae80-fae7-4c45-93eb-b172fb4b8d88"
      },
      "source": [
        "model.to(config.device)\n",
        "print(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MusicTransformer(\n",
            "  (Encoder): EncoderMusic(\n",
            "    (embedding): Embedding(391, 256, padding_idx=388)\n",
            "    (pos_encoding): DynamicPositionEmbedding()\n",
            "    (enc_layers): ModuleList(\n",
            "      (0): EncoderMusicLayer(\n",
            "        (rga): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): EncoderMusicLayer(\n",
            "        (rga): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): EncoderMusicLayer(\n",
            "        (rga): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (Decoder): DecoderMusic(\n",
            "    (embedding): Embedding(391, 256, padding_idx=388)\n",
            "    (pos_encoding): DynamicPositionEmbedding()\n",
            "    (dec_layers): ModuleList(\n",
            "      (0): DecoderMusicLayer(\n",
            "        (rga2): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (rga): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): DecoderMusicLayer(\n",
            "        (rga2): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (rga): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): DecoderMusicLayer(\n",
            "        (rga2): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (rga): RelativeGlobalAttention(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (layernorm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=391, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THNaBSaDGBgn"
      },
      "source": [
        "# load data\n",
        "dataset = DataLoader(config.pickle_dir+\"high/\", config.pickle_dir+\"low/\")\n",
        "# define optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = loss_functions.TransformerLoss()\n",
        "\n",
        "\n",
        "# define tensorboard writer\n",
        "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "train_log_dir = 'logs/'+config.experiment+'/'+current_time+'/train'\n",
        "eval_log_dir = 'logs/'+config.experiment+'/'+current_time+'/eval'\n",
        "train_summary_writer = SummaryWriter(train_log_dir)\n",
        "eval_summary_writer = SummaryWriter(eval_log_dir)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf8V3OJ4F2f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "44a53f7e-de2c-4335-b8d9-b2da3341a69e"
      },
      "source": [
        "# Train Start\n",
        "print(\">> Train start...\")\n",
        "idx = 0\n",
        "for e in range(config.epochs):\n",
        "    print(\">>> [Epoch was updated]\")\n",
        "    for b in range(len(dataset.X) // config.batch_size):\n",
        "        model.train()\n",
        "        batch_x, batch_y = dataset.batch(config.batch_size, config.max_seq, 'train') \n",
        "\n",
        "        # l = max_seq\n",
        "        batch_x = torch.from_numpy(batch_x).contiguous().to(config.device, non_blocking=True, dtype=torch.int)\n",
        "        # l = max_seq+2\n",
        "        batch_y = torch.from_numpy(batch_y).contiguous().to(config.device, non_blocking=True, dtype=torch.int)\n",
        "\n",
        "        # right shifted,  l = max_seq +1\n",
        "        target_inputs = batch_y[:, :-1]\n",
        "        targets = batch_y[:, 1:]\n",
        "\n",
        "        preds = model.forward(batch_x, target_inputs)\n",
        "\n",
        "        loss = criterion(preds, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_summary_writer.add_scalar('loss', loss, global_step=idx)\n",
        "        torch.cuda.empty_cache()\n",
        "        idx += 1\n",
        "\n",
        "        if b % 100 ==0:\n",
        "            model.eval()\n",
        "            eval_x, eval_y = dataset.batch(2, config.max_seq, 'eval')\n",
        "            eval_x = torch.from_numpy(eval_x).contiguous().to(config.device, dtype=torch.int)\n",
        "            eval_y = torch.from_numpy(eval_y).contiguous().to(config.device, dtype=torch.int)\n",
        "\n",
        "            eval_target_inputs = eval_y[:, :-1]\n",
        "            eval_targets = eval_y[:, 1:]\n",
        "\n",
        "            eval_preds = model.forward(eval_x, eval_target_inputs)\n",
        "            eval_loss = criterion(eval_preds, eval_targets)\n",
        "            eval_summary_writer.add_scalar('loss', eval_loss, global_step=idx)\n",
        "\n",
        "            print('\\n====================================================')\n",
        "            print('Epoch:{}/{}'.format(e, config.epochs))\n",
        "            # print('Batch: {}/{}'.format(b, len(dataset.X) // config.batch_size))\n",
        "            print('Train >>>> Loss: {:6.6}'.format(loss))\n",
        "            print('Eval >>>> Loss: {:6.6}'.format(eval_loss))\n",
        "\n",
        "    if e%10 == 0:\n",
        "        torch.save(model.state_dict(), config.model_dir+'/train-{}.pth'.format(e))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), config.model_dir+'/final.pth'.format(idx))\n",
        "eval_summary_writer.close()\n",
        "train_summary_writer.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Train start...\n",
            ">>> [Epoch was updated]\n",
            "\n",
            "====================================================\n",
            "Epoch:0/100\n",
            "Train >>>> Loss: 6.14243\n",
            "Eval >>>> Loss: 6.12814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-04927be4ae6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# l = max_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/Universidad/DTU/Deep_Learning/Project/PianoBot/data_loader.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, batch_size, length, mode, path, min_seq)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpair_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_pair_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m#If it failed, try another file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/Universidad/DTU/Deep_Learning/Project/PianoBot/data_loader.py\u001b[0m in \u001b[0;36m_get_seq\u001b[0;34m(self, pair_file, max_length, pad_token, path, min_seq)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m#print(\"File: \" + str(x_file))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJHOXNR3dC95"
      },
      "source": [
        "To get the training graph it's better to run it on local, so we can access tensorboard easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rMJCLbic9l2",
        "outputId": "eba95112-5cbd-456f-816c-82e99ac3b889"
      },
      "source": [
        "%cd logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'logs'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_58JgVHBdCdr",
        "outputId": "aba59695-4299-4bf7-a80a-7cce1d6d0836"
      },
      "source": [
        "! tensorboard --logdir ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-04 21:50:29.756683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqup4ohkd-yU",
        "outputId": "749402ec-9b74-4937-a0a7-a37c3a8faa02"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Universidad/DTU/Deep_Learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5orr8-GeKbr"
      },
      "source": [
        "To generate a test sequence given the high notes of a random one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pZxCxFshN_2",
        "outputId": "c392ca65-466c-4d62-f4d8-9bff014cfc74"
      },
      "source": [
        "! python generate.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs: [[355 264 371  71 275 199 369  71 305 375  72 256 199 256 370  67 273 373\n",
            "   74 260 200 375  72 261 202 368  74 260 200 371  72 257 195 370  67 271\n",
            "  370  66 274 373  71 259 364  64 258 200 367  72 264 202 373  74 295 194\n",
            "  371  66 272 370  67 271 195 199 192 200 194 259 202 256 372  71 272 375\n",
            "   74 273 375  78 265 199 202 264 374  79 258 206 270 375  76 268 207 259\n",
            "  204 256 373  79 272 373  74 269 207 257 202 256 371  79 275 373  72 273\n",
            "  374  71 274 200 374  72 256 371  66 260 195 266 374  81 274 200 373  72\n",
            "  273 199]]\n",
            "targets: [[355 283 367  55 273 369  59 258 183 269 369  62 290 371  57 328 187 368\n",
            "   59 273 190 371  62 307 185 190 256 187 294 370  59 271 187 277 369  60\n",
            "  270 188 274 370  59 273 187 276 367  57 326 372  55 293 371  59 292 370\n",
            "   62 272 370  54 273 368  52 270 371  50 277 370  49 292 180 372  52 291\n",
            "  185 368  57 292 177 371  49 292 178 372  50 293 182 370  54 271 190 373\n",
            "   62 275 185 370  57 290 371  48 266 183 187 180 177 178 182 190 257 176\n",
            "  281 369  47 370  50 260 185 287 372  55 261 175 284 371  45 256 178 290\n",
            "  369  54]]\n",
            "\u001b[Kgenerating |#################               | 70/128\n",
            "results: [270 366  48 260 366  60 273 188 258 368  60 273 176 256 368  55 273 367\n",
            "  50 260 188 367  60 269 183 259 186 295 370  43 284 171 262 370  52 256\n",
            " 370  55 272 188 278 371  60 260 183 266 180 269 372  43 264 373  48 297\n",
            " 368  52 256 366  55 270 188 171 176 264 180 279 183 355 355 267]\n",
            "info removed pitch: 58\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEWdifBwghOf"
      },
      "source": [
        "The output midi is in /output/generated.mid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD33OW7Bgm72"
      },
      "source": [
        "---Some debug code---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQU3ib9jwrGo",
        "outputId": "22558600-3270-459a-effc-db6f59fccf5e"
      },
      "source": [
        "i=0\n",
        "cut=[]\n",
        "while i<len(seq) and seq[i] != 390:\n",
        "    cut.append(seq[i])\n",
        "    i+=1\n",
        "print(cut)\n",
        "from processor import decode_midi\n",
        "import config\n",
        "decode_midi(cut, file_path=config.save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[261, 291, 272, 295, 62, 294, 283, 43, 60, 190, 55, 190, 61, 62, 190, 171, 280, 54, 300, 58, 59, 266, 55, 62, 31, 292, 31, 183, 256, 257]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pretty_midi.pretty_midi.PrettyMIDI at 0x7f38ef8d85c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJn-wlxlct-K",
        "outputId": "3718f654-9e1f-4f8d-afc4-ca408e2bfd7f"
      },
      "source": [
        "%cd Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Universidad/DTU/Deep_Learning/Project\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}